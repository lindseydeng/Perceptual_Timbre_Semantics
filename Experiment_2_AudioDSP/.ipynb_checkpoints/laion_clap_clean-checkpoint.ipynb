{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ce83bc-3caa-4f88-a91c-3df411d0f3b8",
   "metadata": {},
   "source": [
    "***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ba2eac-54c9-47b1-bfe5-d61ba29b629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from laion_clap import CLAP_Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44efb1c-79db-4d4e-a35c-b4efd2b6aa18",
   "metadata": {},
   "source": [
    "***Utility Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb90cd0b-946d-463c-8789-577f7059f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(x, axis=-1, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Apply L2 normalization to vectors along a given axis.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    return x / (norm + eps)\n",
    "\n",
    "AUDIO_EXT = \".wav\"\n",
    "\n",
    "# Generic regex that matches any <stem>_<type>_<desc>_<scale>.wav\n",
    "GENERIC_AUDIO_RE = re.compile(\n",
    "    r\"^(?P<stem>.+?)_(?P<type>eq|reverb|rvb)_(?P<desc>[A-Za-z0-9\\-]+)_(?P<scale>0\\.\\d+|1(?:\\.0)?)\\.wav$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def parse_audio_files(audio_dir: str, valid_types=None, audio_ext=\".wav\") -> pd.DataFrame:\n",
    "    if not os.path.isdir(audio_dir):\n",
    "        raise FileNotFoundError(f\"❌ Directory does not exist: {audio_dir}\")\n",
    "\n",
    "    if valid_types is not None:\n",
    "        valid_types = set(t.lower() for t in valid_types)\n",
    "\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(audio_dir):\n",
    "        for f in files:\n",
    "            if pathlib.Path(f).suffix.lower() != audio_ext:\n",
    "                continue\n",
    "            m = GENERIC_AUDIO_RE.match(f)\n",
    "            if not m:\n",
    "                continue\n",
    "            file_type = m.group(\"type\").lower()\n",
    "            if valid_types and file_type not in valid_types:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"path\": os.path.join(root, f),\n",
    "                \"type\": file_type,\n",
    "                \"stem\": m.group(\"stem\"),\n",
    "                \"descriptor\": m.group(\"desc\").lower(),\n",
    "                \"scale\": float(m.group(\"scale\"))\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        print(f\"⚠️ No matching files found in: {audio_dir}\")\n",
    "        return pd.DataFrame(columns=[\"path\", \"type\", \"stem\", \"descriptor\", \"scale\"])\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"type\", \"descriptor\", \"scale\", \"path\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify_trend(df_desc):\n",
    "    \"\"\"\n",
    "    Classify the trend of delta_target values across scale levels.\n",
    "\n",
    "    Returns:\n",
    "        str: Trend type (monotonic or peak location)\n",
    "    \"\"\"\n",
    "    g = df_desc.set_index(\"scale\").reindex(EXPECTED_SCALES)\n",
    "    if g[\"delta_target\"].isna().any():\n",
    "        return \"Insufficient data\"\n",
    "\n",
    "    d0, d1, d2 = g[\"delta_target\"].tolist()\n",
    "\n",
    "    # Check monotonicity (with tolerance)\n",
    "    if d0 <= d1 + EPS and d1 <= d2 + EPS:\n",
    "        return \"Monotonic up\"\n",
    "    if d0 >= d1 - EPS and d1 >= d2 - EPS:\n",
    "        return \"Monotonic down\"\n",
    "\n",
    "    # Identify peak location\n",
    "    peak_idx = int(np.argmax([d0, d1, d2]))\n",
    "    return {0: \"Peak low (0.3)\", 1: \"Peak mid (0.6)\", 2: \"Peak high (1.0)\"}[peak_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fe2b2-455a-4bbb-b58d-83cc3fa3dfec",
   "metadata": {},
   "source": [
    "**EQ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9291ea-b275-4fbb-b350-53ae89c0294e",
   "metadata": {},
   "source": [
    "***Configuration***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3164832a-97c2-4c7e-8e93-f88a2989cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of EQ descriptors used in filenames and CLAP embeddings\n",
    "EQ_DESCRIPTORS = [\n",
    "    'warm', 'cold', 'soft', 'loud', 'bright', 'soothing', 'harsh', 'heavy', 'cool',\n",
    "    'smooth', 'calm', 'clear', 'tinny', 'sharp', 'hard', 'crisp', 'mellow', 'dark',\n",
    "    'peaceful', 'gentle'\n",
    "]\n",
    "\n",
    "# === PATHS ===\n",
    "# NOTE: Replace these with actual file paths when running\n",
    "EQ_DIR = \" \"   # Directory containing EQ-modified audio files\n",
    "EQ_ORIGINAL_PATH =  \" \"  # Path to the original unmodified audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a95186-4c51-46ac-83b4-6ea16a41d1fd",
   "metadata": {},
   "source": [
    "***Load Model & Embed Text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5890202-4c68-4072-a3d4-7295103f0a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseydeng/opt/miniconda3/envs/timbre_study/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "laion_eq = CLAP_Module(enable_fusion=False)\n",
    "laion_eq.load_ckpt()  # default pretrained\n",
    "laion_eq.eval()\n",
    "\n",
    "EQ_TEXT_LABELS = EQ_DESCRIPTORS\n",
    "EQ_TEXT_EMBS = laion_eq.get_text_embedding(EQ_TEXT_LABELS, use_tensor=False)  # Shape: [M, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63288d81-5d8d-4e8c-ae5a-75e05ca674f0",
   "metadata": {},
   "source": [
    "***Parse Audio Files***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81c0664-5454-42d9-91d4-1faba0b415a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path    stem descriptor  scale\n",
      "0  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar     bright    0.3\n",
      "1  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar     bright    0.6\n",
      "2  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar     bright    1.0\n",
      "3  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar       calm    0.3\n",
      "4  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar       calm    0.6\n",
      "5  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar       calm    1.0\n",
      "6  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar      clear    0.3\n",
      "7  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar      clear    0.6\n",
      "8  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar      clear    1.0\n",
      "9  /Users/lindseydeng/Desktop/Timbre_Study/timbre...  guitar       cold    0.3\n"
     ]
    }
   ],
   "source": [
    "eq_files_df = parse_audio_files(EQ_DIR, valid_types=[\"eq\"])\n",
    "print(eq_files_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca38c48-1e7f-4726-b523-e6a40c402c65",
   "metadata": {},
   "source": [
    "***Embed Audio***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab302c07-fe19-480e-a893-225e51f0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio embeddings: shape = [1 + N, D]\n",
    "eq_audio_paths = [EQ_ORIGINAL_PATH] + eq_files_df[\"path\"].tolist()\n",
    "eq_audio_embs = laion_eq.get_audio_embedding_from_filelist(x=eq_audio_paths, use_tensor=False)\n",
    "\n",
    "# Split original and manipulated embeddings\n",
    "eq_orig_emb = eq_audio_embs[0:1, :]     # Shape: [1, D]\n",
    "eq_manip_embs = eq_audio_embs[1:, :]    # Shape: [N, D]\n",
    "\n",
    "# Normalize all embeddings (L2 norm)\n",
    "A_orig = l2norm(eq_orig_emb)\n",
    "A_manip = l2norm(eq_manip_embs)\n",
    "T = l2norm(EQ_TEXT_EMBS)  # Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb60e5b-16d4-4de3-82d6-c70509bcc4be",
   "metadata": {},
   "source": [
    "***Compute Similarity Delta***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "678bccd6-9499-4ce7-a68b-2bd7b10710c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>stem</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>scale</th>\n",
       "      <th>sim_target</th>\n",
       "      <th>sim_orig_target</th>\n",
       "      <th>delta_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bright</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.011053</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>-0.012124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bright</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.019404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bright</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.059655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>-0.003487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.010512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>calm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029213</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.027518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>0.007028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>0.031490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076112</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>0.082136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>cold</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.084270</td>\n",
       "      <td>-0.072623</td>\n",
       "      <td>-0.011647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>cold</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.108344</td>\n",
       "      <td>-0.072623</td>\n",
       "      <td>-0.035722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>cold</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.064714</td>\n",
       "      <td>-0.072623</td>\n",
       "      <td>0.007908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path    stem descriptor  \\\n",
       "0   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar     bright   \n",
       "1   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar     bright   \n",
       "2   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar     bright   \n",
       "3   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       calm   \n",
       "4   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       calm   \n",
       "5   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       calm   \n",
       "6   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar      clear   \n",
       "7   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar      clear   \n",
       "8   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar      clear   \n",
       "9   /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       cold   \n",
       "10  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       cold   \n",
       "11  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar       cold   \n",
       "\n",
       "    scale  sim_target  sim_orig_target  delta_target  \n",
       "0     0.3   -0.011053         0.001072     -0.012124  \n",
       "1     0.6    0.020476         0.001072      0.019404  \n",
       "2     1.0    0.060727         0.001072      0.059655  \n",
       "3     0.3   -0.001792         0.001695     -0.003487  \n",
       "4     0.6    0.012207         0.001695      0.010512  \n",
       "5     1.0    0.029213         0.001695      0.027518  \n",
       "6     0.3    0.001004        -0.006024      0.007028  \n",
       "7     0.6    0.025466        -0.006024      0.031490  \n",
       "8     1.0    0.076112        -0.006024      0.082136  \n",
       "9     0.3   -0.084270        -0.072623     -0.011647  \n",
       "10    0.6   -0.108344        -0.072623     -0.035722  \n",
       "11    1.0   -0.064714        -0.072623      0.007908  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original audio similarity to each text descriptor → [M]\n",
    "eq_orig_sims = (A_orig @ T.T)[0]\n",
    "\n",
    "# Manipulated audio similarity to each descriptor → [N, M]\n",
    "eq_manip_sims = A_manip @ T.T\n",
    "\n",
    "# Map descriptor name to index in the text embedding matrix\n",
    "eq_desc2idx = {desc: i for i, desc in enumerate(EQ_TEXT_LABELS)}\n",
    "\n",
    "rows = []\n",
    "for i, row in eq_files_df.iterrows():\n",
    "    desc = row[\"descriptor\"]\n",
    "    idx = eq_desc2idx[desc]\n",
    "    \n",
    "    s_target = float(eq_manip_sims[i, idx])      # Similarity of manipulated audio\n",
    "    s_orig_target = float(eq_orig_sims[idx])     # Similarity of original audio\n",
    "\n",
    "    rows.append({\n",
    "        \"path\": row[\"path\"],\n",
    "        \"stem\": row[\"stem\"],\n",
    "        \"descriptor\": desc,\n",
    "        \"scale\": float(row[\"scale\"]),\n",
    "        \"sim_target\": s_target,\n",
    "        \"sim_orig_target\": s_orig_target,\n",
    "        \"delta_target\": s_target - s_orig_target\n",
    "    })\n",
    "\n",
    "# Final DataFrame of results\n",
    "laion_eq_target_df = pd.DataFrame(rows).sort_values([\"descriptor\", \"scale\"]).reset_index(drop=True)\n",
    "laion_eq_target_df.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf8ba0-eac2-4b32-9eeb-a4e08d1af08e",
   "metadata": {},
   "source": [
    "***Aggregate Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e50a6f13-a979-4ef6-bc82-1980dfc19250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   scale  delta_target\n",
       " 0    0.3      0.017558\n",
       " 1    0.6      0.042074\n",
       " 2    1.0      0.075825,\n",
       "    descriptor  scale  delta_target\n",
       " 0      bright    0.3     -0.012124\n",
       " 1      bright    0.6      0.019404\n",
       " 2      bright    1.0      0.059655\n",
       " 3        calm    0.3     -0.003487\n",
       " 4        calm    0.6      0.010512\n",
       " 5        calm    1.0      0.027518\n",
       " 6       clear    0.3      0.007028\n",
       " 7       clear    0.6      0.031490\n",
       " 8       clear    1.0      0.082136\n",
       " 9        cold    0.3     -0.011647\n",
       " 10       cold    0.6     -0.035722\n",
       " 11       cold    1.0      0.007908\n",
       " 12       cool    0.3      0.038752\n",
       " 13       cool    0.6      0.018034\n",
       " 14       cool    1.0      0.017952\n",
       " 15      crisp    0.3      0.006799\n",
       " 16      crisp    0.6      0.010322\n",
       " 17      crisp    1.0      0.088049\n",
       " 18       dark    0.3      0.043593\n",
       " 19       dark    0.6      0.075318\n",
       " 20       dark    1.0      0.104756\n",
       " 21     gentle    0.3     -0.000326\n",
       " 22     gentle    0.6     -0.027566\n",
       " 23     gentle    1.0     -0.033722\n",
       " 24       hard    0.3      0.044719\n",
       " 25       hard    0.6      0.135827\n",
       " 26       hard    1.0      0.171106\n",
       " 27      harsh    0.3      0.014403\n",
       " 28      harsh    0.6     -0.029543\n",
       " 29      harsh    1.0      0.014681\n",
       " 30      heavy    0.3      0.031064\n",
       " 31      heavy    0.6      0.094498\n",
       " 32      heavy    1.0      0.195044\n",
       " 33       loud    0.3      0.011285\n",
       " 34       loud    0.6      0.159237\n",
       " 35       loud    1.0      0.235959\n",
       " 36     mellow    0.3      0.050455\n",
       " 37     mellow    0.6      0.077450\n",
       " 38     mellow    1.0      0.096140\n",
       " 39   peaceful    0.3      0.009334\n",
       " 40   peaceful    0.6      0.034815\n",
       " 41   peaceful    1.0      0.032850\n",
       " 42      sharp    0.3     -0.012041\n",
       " 43      sharp    0.6      0.009156\n",
       " 44      sharp    1.0      0.159718\n",
       " 45     smooth    0.3      0.050922\n",
       " 46     smooth    0.6      0.089074\n",
       " 47     smooth    1.0      0.111097\n",
       " 48       soft    0.3      0.042922\n",
       " 49       soft    0.6      0.059988\n",
       " 50       soft    1.0      0.086149\n",
       " 51   soothing    0.3      0.024445\n",
       " 52   soothing    0.6      0.074285\n",
       " 53   soothing    1.0      0.087145\n",
       " 54      tinny    0.3     -0.024850\n",
       " 55      tinny    0.6      0.005415\n",
       " 56      tinny    1.0      0.022249\n",
       " 57       warm    0.3      0.039909\n",
       " 58       warm    0.6      0.029477\n",
       " 59       warm    1.0     -0.049888)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall mean delta_target by scale\n",
    "laion_eq_overall = (\n",
    "    laion_eq_target_df\n",
    "    .groupby(\"scale\")[\"delta_target\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(\"scale\")\n",
    ")\n",
    "\n",
    "# Mean delta_target by descriptor and scale\n",
    "laion_eq_by_desc = (\n",
    "    laion_eq_target_df\n",
    "    .groupby([\"descriptor\", \"scale\"])[\"delta_target\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values([\"descriptor\", \"scale\"])\n",
    ")\n",
    "\n",
    "laion_eq_overall, laion_eq_by_desc.head(10)\n",
    "\n",
    "# Save to CSV\n",
    "OUT = \"./outputs/LAION_CLAP\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "laion_eq_overall.to_csv(f\"{OUT}/laionclap_eq_mean_delta_by_scale.csv\", index=False)\n",
    "laion_eq_by_desc.to_csv(f\"{OUT}/laionclap_eq_delta_by_descriptor_and_scale.csv\", index=False)\n",
    "laion_eq_overall, laion_eq_by_desc.head(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c168801-69a5-45ec-bb51-cc196568be64",
   "metadata": {},
   "source": [
    "***Data Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e84aeeb-5e0a-433a-a5e9-e7b07c2f7502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descriptor</th>\n",
       "      <th>trend_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warm</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cool</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gentle</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soothing</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soft</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smooth</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sharp</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mellow</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loud</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>heavy</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bright</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hard</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dark</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crisp</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clear</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>calm</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tinny</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cold</td>\n",
       "      <td>Peak high (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>harsh</td>\n",
       "      <td>Peak high (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>peaceful</td>\n",
       "      <td>Peak mid (0.6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   descriptor       trend_type\n",
       "0        warm   Monotonic down\n",
       "1        cool   Monotonic down\n",
       "2      gentle   Monotonic down\n",
       "3    soothing     Monotonic up\n",
       "4        soft     Monotonic up\n",
       "5      smooth     Monotonic up\n",
       "6       sharp     Monotonic up\n",
       "7      mellow     Monotonic up\n",
       "8        loud     Monotonic up\n",
       "9       heavy     Monotonic up\n",
       "10     bright     Monotonic up\n",
       "11       hard     Monotonic up\n",
       "12       dark     Monotonic up\n",
       "13      crisp     Monotonic up\n",
       "14      clear     Monotonic up\n",
       "15       calm     Monotonic up\n",
       "16      tinny     Monotonic up\n",
       "17       cold  Peak high (1.0)\n",
       "18      harsh  Peak high (1.0)\n",
       "19   peaceful   Peak mid (0.6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "trend_type\n",
       "Monotonic up       14\n",
       "Monotonic down      3\n",
       "Peak high (1.0)     2\n",
       "Peak mid (0.6)      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trend analysis\n",
    "EXPECTED_SCALES = [0.3, 0.6, 1.0]\n",
    "EPS = 1e-6 \n",
    "\n",
    "# Apply trend classification per descriptor\n",
    "trend_rows = []\n",
    "for desc, group in laion_eq_target_df.groupby(\"descriptor\", as_index=False):\n",
    "    trend = classify_trend(group[[\"scale\", \"delta_target\"]].copy())\n",
    "    trend_rows.append({\"descriptor\": desc, \"trend_type\": trend})\n",
    "\n",
    "# Final trend classification DataFrame\n",
    "laion_trend_df = pd.DataFrame(trend_rows).sort_values(\"trend_type\").reset_index(drop=True)\n",
    "display(laion_trend_df)\n",
    "\n",
    "# Count occurrences of each trend type\n",
    "laion_trend_counts = laion_trend_df[\"trend_type\"].value_counts()\n",
    "display(laion_trend_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "755c5e39-6c5b-4b60-9a7b-bd44ab5e347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>scale</th>\n",
       "      <th>sim_target</th>\n",
       "      <th>sim_orig_target</th>\n",
       "      <th>delta_target</th>\n",
       "      <th>path</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bright</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.011053</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>-0.012124</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bright</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.019404</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bright</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.059655</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>-0.003487</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>calm</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.010512</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>calm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029213</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.027518</td>\n",
       "      <td>/Users/lindseydeng/Desktop/timbre_semantics_ex...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model descriptor  scale  sim_target  sim_orig_target  delta_target  \\\n",
       "0  LAION-CLAP     bright    0.3   -0.011053         0.001072     -0.012124   \n",
       "1  LAION-CLAP     bright    0.6    0.020476         0.001072      0.019404   \n",
       "2  LAION-CLAP     bright    1.0    0.060727         0.001072      0.059655   \n",
       "3  LAION-CLAP       calm    0.3   -0.001792         0.001695     -0.003487   \n",
       "4  LAION-CLAP       calm    0.6    0.012207         0.001695      0.010512   \n",
       "5  LAION-CLAP       calm    1.0    0.029213         0.001695      0.027518   \n",
       "\n",
       "                                                path    stem  \n",
       "0  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  \n",
       "1  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  \n",
       "2  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  \n",
       "3  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  \n",
       "4  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  \n",
       "5  /Users/lindseydeng/Desktop/timbre_semantics_ex...  guitar  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>trend_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bright</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>calm</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>clear</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>cold</td>\n",
       "      <td>Peak high (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>cool</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>crisp</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model descriptor       trend_type\n",
       "10  LAION-CLAP     bright     Monotonic up\n",
       "15  LAION-CLAP       calm     Monotonic up\n",
       "14  LAION-CLAP      clear     Monotonic up\n",
       "17  LAION-CLAP       cold  Peak high (1.0)\n",
       "1   LAION-CLAP       cool   Monotonic down\n",
       "13  LAION-CLAP      crisp     Monotonic up"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUT = \"./outputs/LAION_CLAP\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# Attach model tag and standardize columns\n",
    "laion_eq_targets = laion_eq_target_df.copy()\n",
    "laion_eq_targets[\"model\"] = \"LAION-CLAP\"\n",
    "laion_eq_targets = laion_eq_targets[\n",
    "    [\"model\",\"descriptor\",\"scale\",\"sim_target\",\"sim_orig_target\",\"delta_target\",\"path\",\"stem\"]\n",
    "].sort_values([\"descriptor\",\"scale\",\"path\"])\n",
    "\n",
    "laion_eq_trends = laion_trend_df.copy()\n",
    "laion_eq_trends[\"model\"] = \"LAION-CLAP\"\n",
    "laion_eq_trends = laion_eq_trends[[\"model\",\"descriptor\",\"trend_type\"]].sort_values([\"descriptor\"])\n",
    "\n",
    "# Save CSVs\n",
    "laion_eq_targets.to_csv(f\"{OUT}/laionclap_eq_targets.csv\", index=False)\n",
    "laion_eq_trends.to_csv (f\"{OUT}/laionclap_eq_trends.csv\",  index=False)\n",
    "\n",
    "display(laion_eq_targets.head(6))\n",
    "display(laion_eq_trends.head(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad0617-15a5-4883-9a68-342d50365c38",
   "metadata": {},
   "source": [
    "**Reverb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2b884e-141e-4b4b-ad87-cba6473b8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "RVB_DESCRIPTORS = [\n",
    "  'echo', 'distant', 'warm', 'spacious', 'loud', 'deep', \n",
    "    'muffled', 'church', 'big', 'distorted', 'hollow', 'sad', 'soft', \n",
    "    'bass', 'strong', 'low', 'haunting', 'clear','tinny', 'hall']\n",
    "\n",
    "RVB_DIR = \" \" \n",
    "RVB_ORIGINAL_PATH = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ecb3f0-006e-4461-946b-545d04c0b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseydeng/opt/miniconda3/envs/timbre_study/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLAP_Module(\n",
       "  (model): CLAP(\n",
       "    (audio_branch): HTSAT_Swin_Transformer(\n",
       "      (spectrogram_extractor): Spectrogram(\n",
       "        (stft): STFT(\n",
       "          (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "          (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (logmel_extractor): LogmelFilterBank()\n",
       "      (spec_augmenter): SpecAugmentation(\n",
       "        (time_dropper): DropStripes()\n",
       "        (freq_dropper): DropStripes()\n",
       "      )\n",
       "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          dim=96, input_resolution=(64, 64), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=96, window_size=(8, 8), num_heads=4\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=96, window_size=(8, 8), num_heads=4\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(64, 64), dim=96\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=192, input_resolution=(32, 32), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=192, window_size=(8, 8), num_heads=8\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=192, window_size=(8, 8), num_heads=8\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(32, 32), dim=192\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=384, input_resolution=(16, 16), depth=6\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(16, 16), dim=384\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=768, input_resolution=(8, 8), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(8, 8), num_heads=32\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      (tscam_conv): Conv2d(768, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (head): Linear(in_features=527, out_features=527, bias=True)\n",
       "    )\n",
       "    (text_branch): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (text_transform): MLPLayers(\n",
       "      (nonlin): ReLU()\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (text_projection): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (audio_transform): MLPLayers(\n",
       "      (nonlin): ReLU()\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (audio_projection): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laion_rvb = CLAP_Module(enable_fusion=False)\n",
    "laion_rvb.load_ckpt()  # default pretrained\n",
    "laion_rvb.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda9cff4-36aa-4a55-b235-9497af48215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RVB_TEXT_LABELS = RVB_DESCRIPTORS\n",
    "RVB_TEXT_EMBS = laion_rvb.get_text_embedding(RVB_TEXT_LABELS, use_tensor=False)  # [M, D]\n",
    "RVB_TEXT_EMBS.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9d8361-be5e-4c0d-a70e-dafc80cda210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                path    stem descriptor  scale\n",
       " 0  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass    0.3\n",
       " 1  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass    0.6\n",
       " 2  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass    1.0\n",
       " 3  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big    0.3\n",
       " 4  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big    0.6\n",
       " 5  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big    1.0\n",
       " 6  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church    0.3\n",
       " 7  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church    0.6\n",
       " 8  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church    1.0\n",
       " 9  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar      clear    0.3,\n",
       " 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvb_files_df = parse_audio_files(RVB_DIR, valid_types=[\"reverb\", \"rvb\"])\n",
    "rvb_files_df.head(10), len(rvb_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5fdf5f-e8f5-4e68-ba3b-69e65a524c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 512), (1, 512))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvb_audio_paths = [RVB_ORIGINAL_PATH] + rvb_files_df[\"path\"].tolist()\n",
    "rvb_audio_embs = laion_rvb.get_audio_embedding_from_filelist(x=rvb_audio_paths, use_tensor=False)  # [1+N, D]\n",
    "rvb_orig_emb = rvb_audio_embs[0:1, :]     # [1, D]\n",
    "rvb_manip_embs = rvb_audio_embs[1:, :]    # [N, D]\n",
    "rvb_manip_embs.shape, rvb_orig_emb.shape\n",
    "\n",
    "# Normalize\n",
    "A_orig = l2norm(rvb_orig_emb)          # [1, D]\n",
    "A_manip = l2norm(rvb_manip_embs)       # [N, D]\n",
    "T = l2norm(RVB_TEXT_EMBS)              # [M, D]\n",
    "\n",
    "rvb_manip_embs.shape, rvb_orig_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2000f738-e6d7-457d-a9ae-4afb4d7e0582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>stem</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>scale</th>\n",
       "      <th>sim_target</th>\n",
       "      <th>sim_orig_target</th>\n",
       "      <th>delta_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bass</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.029729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bass</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.118151</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.038126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>bass</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>-0.060867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>big</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.027178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>big</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.091153</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.046150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>big</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.034541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>church</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.025536</td>\n",
       "      <td>-0.079649</td>\n",
       "      <td>0.054112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>church</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>-0.079649</td>\n",
       "      <td>0.069786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>church</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.063787</td>\n",
       "      <td>-0.079649</td>\n",
       "      <td>0.143436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.015447</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.015149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.011591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "      <td>clear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.004892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path    stem descriptor  \\\n",
       "0   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass   \n",
       "1   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass   \n",
       "2   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar       bass   \n",
       "3   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big   \n",
       "4   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big   \n",
       "5   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar        big   \n",
       "6   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church   \n",
       "7   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church   \n",
       "8   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar     church   \n",
       "9   //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar      clear   \n",
       "10  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar      clear   \n",
       "11  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar      clear   \n",
       "\n",
       "    scale  sim_target  sim_orig_target  delta_target  \n",
       "0     0.3    0.109753         0.080024      0.029729  \n",
       "1     0.6    0.118151         0.080024      0.038126  \n",
       "2     1.0    0.019157         0.080024     -0.060867  \n",
       "3     0.3    0.072180         0.045002      0.027178  \n",
       "4     0.6    0.091153         0.045002      0.046150  \n",
       "5     1.0    0.079543         0.045002      0.034541  \n",
       "6     0.3   -0.025536        -0.079649      0.054112  \n",
       "7     0.6   -0.009863        -0.079649      0.069786  \n",
       "8     1.0    0.063787        -0.079649      0.143436  \n",
       "9     0.3    0.015447         0.000297      0.015149  \n",
       "10    0.6    0.011889         0.000297      0.011591  \n",
       "11    1.0    0.005189         0.000297      0.004892  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original audio similarity to each text descriptor → [M]\n",
    "rvb_orig_sims = (A_orig @ T.T)[0]\n",
    "\n",
    "# Manipulated audio similarity to each descriptor → [N, M]\n",
    "rvb_manip_sims = A_manip @ T.T\n",
    "\n",
    "# Map descriptor name to index in the text embedding matrix\n",
    "rvb_desc2idx = {d:i for i, d in enumerate(RVB_TEXT_LABELS)}\n",
    "rows = []\n",
    "for i, r in rvb_files_df.iterrows():\n",
    "    desc = r[\"descriptor\"]\n",
    "    idx = rvb_desc2idx[desc]\n",
    "    s_target = float(rvb_manip_sims[i, idx])\n",
    "    s_orig_target = float(rvb_orig_sims[idx])\n",
    "    rows.append({\n",
    "        \"path\": r[\"path\"],\n",
    "        \"stem\": r[\"stem\"],\n",
    "        \"descriptor\": desc,\n",
    "        \"scale\": float(r[\"scale\"]),\n",
    "        \"sim_target\": s_target,           # cosine(sim) in [-1, 1]\n",
    "        \"sim_orig_target\": s_orig_target,\n",
    "        \"delta_target\": s_target - s_orig_target\n",
    "    })\n",
    "\n",
    "laion_rvb_target_df = pd.DataFrame(rows).sort_values([\"descriptor\",\"scale\"]).reset_index(drop=True)\n",
    "laion_rvb_target_df.head(12)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759a6993-0a03-449b-b51e-2a9ecc9061d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   scale  delta_target\n",
       " 0    0.3      0.024491\n",
       " 1    0.6      0.036284\n",
       " 2    1.0      0.090863,\n",
       "    descriptor  scale  delta_target\n",
       " 0        bass    0.3      0.029729\n",
       " 1        bass    0.6      0.038126\n",
       " 2        bass    1.0     -0.060867\n",
       " 3         big    0.3      0.027178\n",
       " 4         big    0.6      0.046150\n",
       " 5         big    1.0      0.034541\n",
       " 6      church    0.3      0.054112\n",
       " 7      church    0.6      0.069786\n",
       " 8      church    1.0      0.143436\n",
       " 9       clear    0.3      0.015149\n",
       " 10      clear    0.6      0.011591\n",
       " 11      clear    1.0      0.004892\n",
       " 12       deep    0.3      0.047259\n",
       " 13       deep    0.6      0.064997\n",
       " 14       deep    1.0      0.258337\n",
       " 15    distant    0.3      0.022482\n",
       " 16    distant    0.6      0.077123\n",
       " 17    distant    1.0      0.133342\n",
       " 18  distorted    0.3     -0.039165\n",
       " 19  distorted    0.6     -0.058085\n",
       " 20  distorted    1.0      0.049977\n",
       " 21       echo    0.3      0.012167\n",
       " 22       echo    0.6      0.014727\n",
       " 23       echo    1.0      0.084676\n",
       " 24       hall    0.3      0.017674\n",
       " 25       hall    0.6      0.032087\n",
       " 26       hall    1.0      0.096946\n",
       " 27   haunting    0.3      0.024818\n",
       " 28   haunting    0.6      0.050044\n",
       " 29   haunting    1.0      0.119048\n",
       " 30     hollow    0.3      0.041008\n",
       " 31     hollow    0.6      0.082673\n",
       " 32     hollow    1.0      0.215316\n",
       " 33       loud    0.3      0.067560\n",
       " 34       loud    0.6      0.015575\n",
       " 35       loud    1.0      0.036125\n",
       " 36        low    0.3      0.059138\n",
       " 37        low    0.6      0.079515\n",
       " 38        low    1.0      0.201022\n",
       " 39    muffled    0.3     -0.027336\n",
       " 40    muffled    0.6     -0.043407\n",
       " 41    muffled    1.0     -0.031312\n",
       " 42        sad    0.3      0.026064\n",
       " 43        sad    0.6      0.002326\n",
       " 44        sad    1.0      0.053480\n",
       " 45       soft    0.3      0.014238\n",
       " 46       soft    0.6      0.036311\n",
       " 47       soft    1.0      0.072430\n",
       " 48   spacious    0.3      0.006637\n",
       " 49   spacious    0.6      0.077872\n",
       " 50   spacious    1.0      0.219948\n",
       " 51     strong    0.3      0.025644\n",
       " 52     strong    0.6      0.028657\n",
       " 53     strong    1.0      0.048093\n",
       " 54      tinny    0.3      0.028634\n",
       " 55      tinny    0.6      0.064007\n",
       " 56      tinny    1.0      0.153159\n",
       " 57       warm    0.3      0.036828\n",
       " 58       warm    0.6      0.035606\n",
       " 59       warm    1.0     -0.015334)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laion_rvb_overall = (\n",
    "    laion_rvb_target_df.groupby(\"scale\")[\"delta_target\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(\"scale\")\n",
    ")\n",
    "laion_rvb_by_desc = (\n",
    "    laion_rvb_target_df.groupby([\"descriptor\",\"scale\"])[\"delta_target\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values([\"descriptor\",\"scale\"])\n",
    ")\n",
    "\n",
    "laion_rvb_overall, laion_rvb_by_desc.head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65404c9f-0287-4edc-9a04-025d3b74fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descriptor</th>\n",
       "      <th>trend_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warm</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clear</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strong</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacious</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soft</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>low</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hollow</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tinny</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>haunting</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>echo</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>distant</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>deep</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>church</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hall</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>distorted</td>\n",
       "      <td>Peak high (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sad</td>\n",
       "      <td>Peak high (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>loud</td>\n",
       "      <td>Peak low (0.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>muffled</td>\n",
       "      <td>Peak low (0.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>big</td>\n",
       "      <td>Peak mid (0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bass</td>\n",
       "      <td>Peak mid (0.6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   descriptor       trend_type\n",
       "0        warm   Monotonic down\n",
       "1       clear   Monotonic down\n",
       "2      strong     Monotonic up\n",
       "3    spacious     Monotonic up\n",
       "4        soft     Monotonic up\n",
       "5         low     Monotonic up\n",
       "6      hollow     Monotonic up\n",
       "7       tinny     Monotonic up\n",
       "8    haunting     Monotonic up\n",
       "9        echo     Monotonic up\n",
       "10    distant     Monotonic up\n",
       "11       deep     Monotonic up\n",
       "12     church     Monotonic up\n",
       "13       hall     Monotonic up\n",
       "14  distorted  Peak high (1.0)\n",
       "15        sad  Peak high (1.0)\n",
       "16       loud   Peak low (0.3)\n",
       "17    muffled   Peak low (0.3)\n",
       "18        big   Peak mid (0.6)\n",
       "19       bass   Peak mid (0.6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "trend_type\n",
       "Monotonic up       12\n",
       "Monotonic down      2\n",
       "Peak high (1.0)     2\n",
       "Peak low (0.3)      2\n",
       "Peak mid (0.6)      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#trend analysis\n",
    "EXPECTED_SCALES = [0.3, 0.6, 1.0]\n",
    "EPS = 1e-6 \n",
    "\n",
    "# Apply trend classification per descriptor\n",
    "trend_rows = []\n",
    "for desc, group in laion_rvb_target_df.groupby(\"descriptor\", as_index=False):\n",
    "    trend = classify_trend(group[[\"scale\", \"delta_target\"]].copy())\n",
    "    trend_rows.append({\"descriptor\": desc, \"trend_type\": trend})\n",
    "\n",
    "# Final trend classification DataFrame\n",
    "laion_trend_df = pd.DataFrame(trend_rows).sort_values(\"trend_type\").reset_index(drop=True)\n",
    "display(laion_trend_df)\n",
    "\n",
    "# Count occurrences of each trend type\n",
    "laion_trend_counts = laion_trend_df[\"trend_type\"].value_counts()\n",
    "display(laion_trend_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c1f67b6-c7db-40bf-b4f2-c42005767141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>scale</th>\n",
       "      <th>sim_target</th>\n",
       "      <th>sim_orig_target</th>\n",
       "      <th>delta_target</th>\n",
       "      <th>path</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bass</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.029729</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bass</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.118151</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bass</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>0.080024</td>\n",
       "      <td>-0.060867</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>big</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>big</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.091153</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.046150</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>big</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>//Users/lindseydeng/Desktop/timbre_semantics_e...</td>\n",
       "      <td>guitar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model descriptor  scale  sim_target  sim_orig_target  delta_target  \\\n",
       "0  LAION-CLAP       bass    0.3    0.109753         0.080024      0.029729   \n",
       "1  LAION-CLAP       bass    0.6    0.118151         0.080024      0.038126   \n",
       "2  LAION-CLAP       bass    1.0    0.019157         0.080024     -0.060867   \n",
       "3  LAION-CLAP        big    0.3    0.072180         0.045002      0.027178   \n",
       "4  LAION-CLAP        big    0.6    0.091153         0.045002      0.046150   \n",
       "5  LAION-CLAP        big    1.0    0.079543         0.045002      0.034541   \n",
       "\n",
       "                                                path    stem  \n",
       "0  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  \n",
       "1  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  \n",
       "2  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  \n",
       "3  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  \n",
       "4  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  \n",
       "5  //Users/lindseydeng/Desktop/timbre_semantics_e...  guitar  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>trend_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>bass</td>\n",
       "      <td>Peak mid (0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>big</td>\n",
       "      <td>Peak mid (0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>church</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>clear</td>\n",
       "      <td>Monotonic down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>deep</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LAION-CLAP</td>\n",
       "      <td>distant</td>\n",
       "      <td>Monotonic up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model descriptor      trend_type\n",
       "19  LAION-CLAP       bass  Peak mid (0.6)\n",
       "18  LAION-CLAP        big  Peak mid (0.6)\n",
       "12  LAION-CLAP     church    Monotonic up\n",
       "1   LAION-CLAP      clear  Monotonic down\n",
       "11  LAION-CLAP       deep    Monotonic up\n",
       "10  LAION-CLAP    distant    Monotonic up"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUT = \"./outputs/LAION_CLAP\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# Attach model tag and standardize columns\n",
    "laion_rvb_targets = laion_rvb_target_df.copy()\n",
    "laion_rvb_targets[\"model\"] = \"LAION-CLAP\"\n",
    "laion_rvb_targets = laion_rvb_targets[\n",
    "    [\"model\",\"descriptor\",\"scale\",\"sim_target\",\"sim_orig_target\",\"delta_target\",\"path\",\"stem\"]\n",
    "].sort_values([\"descriptor\",\"scale\",\"path\"])\n",
    "\n",
    "laion_rvb_trends = laion_trend_df.copy()\n",
    "laion_rvb_trends[\"model\"] = \"LAION-CLAP\"\n",
    "laion_rvb_trends = laion_rvb_trends[[\"model\",\"descriptor\",\"trend_type\"]].sort_values([\"descriptor\"])\n",
    "\n",
    "# Save CSVs\n",
    "laion_rvb_targets.to_csv(f\"{OUT}/laionclap_rvb_targets.csv\", index=False)\n",
    "laion_rvb_trends.to_csv (f\"{OUT}/laionclap_rvb_trends.csv\",  index=False)\n",
    "\n",
    "display(laion_rvb_targets.head(6))\n",
    "display(laion_rvb_trends.head(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15351473-1450-4c7e-aa1e-5163df254f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
